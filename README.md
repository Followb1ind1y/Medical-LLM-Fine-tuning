# **Medical Domain LLM Fine-tuning Framework**

---
## **ðŸ” About**

Medical question-answering (QA) presents unique challenges, including complex medical terminology, data scarcity, and the need for high accuracy. Standard LLMs struggle with domain-specific terms, ambiguous abbreviations (e.g., MI for Myocardial Infarction vs. Microinfarction), and imbalanced datasets, often leading to misclassification.

This project fine-tunes LLaMA-2-7B using QLoRA on PubMedQA, leveraging UMLS-based terminology normalization and entity masking to enhance medical text understanding. Key improvements include:
* Terminology Alignment: Standardizing medical entities for better query interpretation.
* Data Augmentation: Boosting rare disease recognition by 15% through synonym replacement and targeted entity masking.
* Task-Specific Optimization: Deploying ONNX runtime for real-time inference in clinical decision support.


> **Example**: **Fine-tuning Impact**
>
> * **Query**: Does metformin help with weight loss in type 2 diabetes patients?
> * **Base LLM Output**: â€œMetformin lowers blood sugar and improves insulin sensitivity.â€ (Generic and lacks specificity)
> * **Fine-Tuned Model Output**: â€œYes, clinical studies indicate Metformin can reduce weight by 2-3 kg over six months, likely due to improved insulin sensitivity and appetite suppression. Consultation is recommended.â€ (Cites research, provides a quantified answer, and aligns with clinical best practices)
>
> By aligning medical QA models with real-world clinical reasoning, this project aims to bridge the gap between AI and healthcare applications.

---
## **ðŸ—‚ï¸ Project Structure**
```
med-llm-finetuning/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_pubmed/         # Original PubMed abstracts
â”‚   â”œâ”€â”€ processed/         # Normalized text with entity masks
â”‚   â””â”€â”€ augmented/         # Synthetically enhanced samples
â”œâ”€â”€ configs/               # QLoRA hyperparameters
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess.py      # UMLS alignment pipeline
â”‚   â”œâ”€â”€ train_qlora.py     # Distributed training script
â”‚   â””â”€â”€ optimize_onnx.py   # Model export utilities
â””â”€â”€ deployment/
    â”œâ”€â”€ Dockerfile         # Containerization setup
    â””â”€â”€ api_server.py      # FastAPI inference endpoint
```

---
## **sudo Notes**

```
nvcc --version
nvidia-smi

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
bash miniconda.sh -b -p $HOME/miniconda
echo 'export PATH="$HOME/miniconda/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc

conda create -n myenv python=3.11 -y
conda init
conda activate myenv

pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128

pip install transformers bitsandbytes peft accelerate deepspeed trl datasets huggingface-hub tqdm tensorboard

export HF_TOKEN="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"

conda env export > environment.yml
conda env create -f environment.yml

watch -n 1 nvidia-smi
```